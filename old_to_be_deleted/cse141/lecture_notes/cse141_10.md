# CSE141 10: Memory Subsystem Design

To view the original lecture notes provided by the professor, please visit at [this link](files/cse141_10.pdf).


## Quick Summary
- Up to this point, we’ve been assuming memory can be 
accessed in a single cycle.
- In fact, that was true once (a long time ago...).  But CPU 
cycle time has decreased rapidly, while memory access 
time has decreased very little.
- In modern computers, memory latency can be in the 
neighborhood of 250-500 cycles!

## The truth about memory latency
- So then what is the point of pipelining, branch prediction, 
etc. if memory latency is 300 cycles?

- Keep in mind, 20% of instructions are loads and stores, 
and we fetch (read inst memory) every instruction.

- That is assuming DRAM technology, which is necessary 
for large main memories (multiple gigabytes, for example)

- But we can design much smaller (capacity) memories 
using SRAM, even on chip.

- If we still want to access it in a cycle, it should be KB, not 
MB or GB.

## Memory Locality
- Memory hierarchies take advantage of memory locality. 
- Memory locality is the principle that future memory 
accesses are near past accesses.
- Memories take advantage of two types of locality
  -  near in time  => we will often access the 
same data again very soon
  -  near in space/distance => our next access is 
- often very close to our last access (or recent accesses).
(this sequence of addresses exhibits both temporal and spatial locality)
1,2,3,1,2,3,8,8,47,9,10,8,8...

## Locality and cacheing
- Memory hierarchies exploit locality by cacheing (keeping 
close to the processor) data likely to be used again.
- This is done because we can build large, slow memories 
and small, fast memories, but we can’t build large, fast 
memories.
- If it works, we get the illusion of SRAM access time with 
disk capacity
    - SRAM access times are ~1ns at cost of ~$500 per Gbyte.
    - DRAM access times are ~60ns at cost of ~$10 per Gbyte.
    - Disk access times are 5 to 20 million ns at cost of $.20 to $2 per Gbyte.

## Cache Fundamentals
- cache hit -- an access where the data is 
found in the cache.
- cache miss -- an access which isn’t
- hit time -- time to access the cache
- miss penalty -- time to move data from 
further level to closer
- hit ratio -- fraction of accesses where the 
data is found in the cache
-  miss ratio -- (1 - hit ratio)
-  cache block size or cache line size– the 
amount of data that gets transferred on a  
cache miss.
- instruction cache -- cache that only 
holds instructions.
- data cache -- cache that only caches 
data.
- unified cache -- cache that holds both.

## Cacheing Issues
- n a memory access -
    - How do I know if this is a hit or miss?
- On a cache miss -
    -  where to put the new data?
    - what data to throw out?
    - how to remember what data this is?

### A simple cache
- A cache that can put a line of data anywhere is called fully associative cache.
- The most popular replacement strategy is LRU(Least Recently Used)

### A simpler cache
- A cache that can put a line of data in exactly one place is called a direct-mapped cache

### A set-associative cache
- A cache that can put a line of data in exactly n places is 
called n-way set-associative.
- The cache lines/blocks that share the same index are a 
cache set.

### Longer Cache Blocks
- Large cache blocks take advantage of spatial locality.
- Too large of a block size can waste cache space.
- Longer cache blocks require less tag space

## Cache Parameters
- Cache size = Number of sets * block size * associativity

## Handling a Cache Access
1. Use index and tag to access cache and determine hit/miss.
2.  If hit, return requested data.
3.  If miss, select a cache block to be replaced, and access 
memory or next lower cache (possibly stalling the 
processor).
    - load entire missed cache line into cache
    - return requested data to CPU (or higher cache)
4.  If next lower memory is a cache, goto step 1 for that 
cache.

## Associative Caches
- Higher hit rates, but...
- longer access time (longer to determine hit/miss, more 
muxing of outputs)
- more space (longer tags)
    - 16 KB, 16-byte blocks, dm, tag = ?
    - 16 KB, 16-byte blocks, 4-way, tag = ?

## Dealing with Stores
- Stores must be handled differently than loads, because...
    - they don’t necessarily require the CPU to stall.
    - they change the content of cache/memory (creating memory 
consistency issues)
    - may require a                and a write to memory to complete

### Policy decisions for stores
- Keep memory and cache identical?
    - => all writes go to both cache and main memory
    - => writes go only to cache.  Modified cache 
lines are written back to memory when the line is replaced.
- Make room in cache for store miss?
    - write-allocate => on a store miss, bring written line into the cache
    - write-around => on a store miss, ignore cache
- On a store hit, write the new data to cache. In a write-
through cache, write the data immediately to memory.  In a 
write-back cache, mark the line as dirty.
- On a store miss, initiate a cache block load from memory 
for a write-allocate cache.  Write directly to memory for a 
write-around cache.
- On any kind of cache miss in a write-back cache, if the line 
to be replaced in the cache is dirty, write it back to 
memory. 

## Cache Performance
- CPI = BCPI + MCPI
    - BCPI = base CPI, which means the CPI assuming perfect memory
    - MCPI = the memory CPI, the number of cycles (per instruction) the 
processor is stalled waiting for memory.
- MCPI = accesses/instruction * miss rate * miss penalty
    - this assumes we stall the pipeline on both read and write misses, that the 
miss penalty is the same for both, that cache hits require no stalls.
    - If the miss penalty or miss rate is different for Inst cache and data cache 
(common case), then
    - MCPI = I$ accesses/inst*I$MR*I$MP + D$ acc/inst*D$MR*D$MP

- CPI = BCPI + DHSPI + BHSPI + MCPI
    - DHSPI = data hazard stalls per instruction
    - BHSPI = branch hazard stalls per instruction.

## Cache Alignment
- The data that gets moved into the cache on a miss 
are all data whose addresses share the same tag and 
index (regardless of which data gets accessed first).
- This results in 
    - no overlap of cache lines
    - easy mapping of addresses to cache lines (no additions)
    - data at address X always being present in the same 
location in the cache block (at byte X mod blocksize) if it is 
there at all.
- Think of main memory as organized into cache-line 
sized pieces (because in reality, it is!).

### Three types of cache misses
- Compulsory (or cold-start) misses
    - first access to the data.
- Capacity misses
    – we missed only because the cache isn’t big enough.
- Conflict misses
    – we missed because the data maps to the same line as other data that 
forced it out of the cache.

## Key Points
- Caches give illusion of a large, cheap memory with the 
access time of a fast, expensive memory.
- Caches take advantage of memory locality, specifically 
temporal locality and spatial locality.
- Cache design presents many options (block size, cache 
size, associativity, write policy) that an architect must 
combine to minimize miss rate and access time to 
maximize performance.
